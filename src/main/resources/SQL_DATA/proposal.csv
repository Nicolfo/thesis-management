id,title,supervisor_id,coSupervisors_ids,keywords,type,groups_ids,description,requiredKnowledge,notes,expiration,level,CdS
2,A Mobile App for Remote Telepathology,300020,"300018,300015","Mobile Applications, Artificial Intelligence, Human-Computer Interfaces, Machine Learning", Software Testing,"GRAINS","Many End-to-End (E2E) testing tools allow developers to create repeatable test scripts that mimic a human user's interaction with the application and evaluate its response. Various paradigms of testing tools have been defined that are differentiated based upon the way the widgets are located on the GUI (e.g., text, images, layout properties). This thesis falls within the visual GUI testing paradigm scope, where screen snapshots used as visual locators:\n- identify the widgets, and\n- replicate an existing test suite across multiple devices or versions of the same app.\n\nThis technique leverages image recognition algorithms.\n\nIn particular, the thesis focuses on testing mobile applications where the same widgets have to be identified across multiple devices, with different screen sizes and characteristics. In this context, marginal variations in the graphical rendering can invalidate widgets' recognition if the image recognition algorithm is not sufficiently robust. This thesis aims to extend an existing prototype for visual GUI testing by leveraging deep learning. In particular, the following activities are envisioned:\n- Training a detector to identify widgets and their classes with high recall.\n- Using the features extracted by the neural network to match the same widgets when the app is rendered across multiple devices.\n- Investigating the possibility of integrating text extraction with image analysis.\n\nThe solution will be characterized and possibly optimized in terms of accuracy and execution speed. The developed techniques will be included in an open-source library.\nThe student should possess or should be willing to acquire these skills:\n- programming skills (Python, deep learning frameworks);\n- experience in training deep neural networks;\n- fundamentals of mobile development (Android GUI, the Android Studio development environment).","programming skills (Python, deep learning frameworks), experience in training deep neural networks, fundamentals of mobile development (Android GUI, the Android Studio development environment)","See also: http://grains.polito.it/work.php","2024-07-19T22:00:00.000+00:00","Master's","Electrical Engineering"
3,"3D Reconstruction and VR visualization of nuclear scattering events",300011,"300012,300014","Mobile Applications, Artificial Intelligence, Human-Computer Interfaces, Machine Learning", Software Testing,"APPEAL","Many End-to-End (E2E) testing tools allow developers to create repeatable test scripts that mimic a human user's interaction with the application and evaluate its response. Various paradigms of testing tools have been defined that are differentiated based upon the way the widgets are located on the GUI (e.g., text, images, layout properties). This thesis falls within the visual GUI testing paradigm scope, where screen snapshots used as visual locators:\n- identify the widgets, and\n- replicate an existing test suite across multiple devices or versions of the same app.\n\nThis technique leverages image recognition algorithms.\n\nIn particular, the thesis focuses on testing mobile applications where the same widgets have to be identified across multiple devices, with different screen sizes and characteristics. In this context, marginal variations in the graphical rendering can invalidate widgets' recognition if the image recognition algorithm is not sufficiently robust. This thesis aims to extend an existing prototype for visual GUI testing by leveraging deep learning. In particular, the following activities are envisioned:\n- Training a detector to identify widgets and their classes with high recall.\n- Using the features extracted by the neural network to match the same widgets when the app is rendered across multiple devices.\n- Investigating the possibility of integrating text extraction with image analysis.\n\nThe solution will be characterized and possibly optimized in terms of accuracy and execution speed. The developed techniques will be included in an open-source library.\nThe student should possess or should be willing to acquire these skills:\n- programming skills (Python, deep learning frameworks);\n- experience in training deep neural networks;\n- fundamentals of mobile development (Android GUI, the Android Studio development environment).","programming skills (Python, deep learning frameworks), experience in training deep neural networks, fundamentals of mobile development (Android GUI, the Android Studio development environment)","See also: http://grains.polito.it/work.php","2024-07-19T22:00:00.000+00:00","Bachelor's","Computer Engineering"
4,"Computer vision techniques for mobile testing",300016,"300014,300013","Mobile Applications, Artificial Intelligence, Human-Computer Interfaces, Machine Learning", Software Testing,"LIM","Many End-to-End (E2E) testing tools allow developers to create repeatable test scripts that mimic a human user's interaction with the application and evaluate its response. Various paradigms of testing tools have been defined that are differentiated based upon the way the widgets are located on the GUI (e.g., text, images, layout properties). This thesis falls within the visual GUI testing paradigm scope, where screen snapshots used as visual locators:\n- identify the widgets, and\n- replicate an existing test suite across multiple devices or versions of the same app.\n\nThis technique leverages image recognition algorithms.\n\nIn particular, the thesis focuses on testing mobile applications where the same widgets have to be identified across multiple devices, with different screen sizes and characteristics. In this context, marginal variations in the graphical rendering can invalidate widgets' recognition if the image recognition algorithm is not sufficiently robust. This thesis aims to extend an existing prototype for visual GUI testing by leveraging deep learning. In particular, the following activities are envisioned:\n- Training a detector to identify widgets and their classes with high recall.\n- Using the features extracted by the neural network to match the same widgets when the app is rendered across multiple devices.\n- Investigating the possibility of integrating text extraction with image analysis.\n\nThe solution will be characterized and possibly optimized in terms of accuracy and execution speed. The developed techniques will be included in an open-source library.\nThe student should possess or should be willing to acquire these skills:\n- programming skills (Python, deep learning frameworks);\n- experience in training deep neural networks;\n- fundamentals of mobile development (Android GUI, the Android Studio development environment).","programming skills (Python, deep learning frameworks), experience in training deep neural networks, fundamentals of mobile development (Android GUI, the Android Studio development environment)","See also: http://grains.polito.it/work.php","2024-07-19T22:00:00.000+00:00","Master's","Computer Engineering"